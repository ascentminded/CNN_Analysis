{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNCZg69bFrPH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Building the model using Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#Additional torch stuff, like loss functions:\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import tensorflow as tf # To get the dataset\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "#For train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#For plotting\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Pre-processing"
      ],
      "metadata": {
        "id": "R9YDaBPLI2Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm using the MNIST dataset, and loading it using tensorflow's datasets package.\n",
        "You can obviously use any other dataset, but it will affect a few key things:\n",
        "- n_classes when we're defining the model. It's currently set to 10, since there are 10 classes (digits) in MNIST, but will vary with dataset.\n",
        "- Loss. This, as well as the above will change if you're not classifying data. The loss *'criterion'* can be swapped for BCELoss (Binary cross-entropy loss as opposed to normal Cross-Entropy loss). Also, n_classes should be set to 1.\n",
        "- Potentially sigmoid(x) instead of softmax(x) at the end of the model."
      ],
      "metadata": {
        "id": "C4xUOTP3aEIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
        "x_train = np.expand_dims(x_train, axis=-1)   # Add channel dimension\n",
        "\n",
        "# Create masks (e.g., binary segmentation of digits)\n",
        "# Here, we'll use the digits as masks for demonstration purposes.\n",
        "y_train = (y_train[..., None] > 0).astype(\"float32\")  # Convert labels to binary masks\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH5WQZ-bh407",
        "outputId": "d909b2bc-491f-4266-8cb0-1eb3e9b8dfbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOQT9ItoJEgy",
        "outputId": "72916599-e178-47be-d94a-d5955e060e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "# Ensure Y_train and Y_test have 4 dimensions\n",
        "# Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
        "# Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)    # Add channel dimension\n",
        "\n",
        "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)  # No unsqueeze for labels\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "7XH1UNRti-Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "nPkiKrjTH5jE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A convolutional neural network essentially is a specialised case of a neural network for image analysis. Since the number of weights would grow exponentially with dimensions of the image in a typical fully connected neural net, CNNs were developed. CNNs use a number of filters, which slice up the image into overlapping chunks and analyse each as one unit.\n",
        "The CNN will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as 32x32x12 if we decided to use 12 filters on a say 32x32x3 image.\n",
        "\n",
        "A ReLU or other activation function will likely follow, integrating some non-linearity, but leaving the dimensions unchanged"
      ],
      "metadata": {
        "id": "TXd7EnAYYyoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of a basic CONVNET would go something like\n",
        "[Input] - *[Conv] - [ReLU] -[Conv] - [ReLU]* - [Pool] - *[Conv] - [ReLU] - [Conv] - [ReLU] *- [Pool] - *[Conv] - [ReLU] - [Conv] - [ReLU]* - [Pool] -- [FC] - Output"
      ],
      "metadata": {
        "id": "Mt7a5jzZaf5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The POOL operation downsamples the image along width and height, so 36x36x12 will become 18x18x12 for instance.\n",
        "A Fully Connected (FC) layer is used at the end to compute scores for instance."
      ],
      "metadata": {
        "id": "v6AMsRQbZxwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A UNet model contains ~ 4 downblocks, a bottleneck and 4 upblocks![UNet arch.PNG](https://miro.medium.com/v2/resize:fit:933/1*hzufVrlQYVs_Lj-vHw8SgQ.png)"
      ],
      "metadata": {
        "id": "m17ZHbnLhFyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do we pick parameters?**\n",
        "- Obviously it varies with the dataset, which is why I've kept the parameters as global variables that can be changed neatly outside of the confusing looking model code\n",
        "- However, there are some guidelines that we can keep in mind:\n",
        "  - Starting with a small number of filters during the downward steps and increasing until it reaches the bottleneck\n",
        "  - Using powers of two for these filters\n",
        "  - Matching the filter sizes, in reverse, for the upward steps\n",
        "  - For kernel size, 3×3 is the most common choice for convolution layers in UNet. It provides a balance between capturing local details and computational efficiency. 1×1 convolutions can be used at the output layer for final predictions."
      ],
      "metadata": {
        "id": "sNnIbVMVnhcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialising some constants:\n",
        "p_kernel = (2,2)\n",
        "p_stride = (2,2)\n",
        "strides = 1\n",
        "\n",
        "# filters = [16, 32, 64, 128, 256]\n",
        "input_ch = 1\n",
        "filters = [64, 128, 256, 512, 1024]\n",
        "kernel = (3,3)\n",
        "learning_rate = 1e-4"
      ],
      "metadata": {
        "id": "JaxRJrvVi3VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually the max pool kernel and stride are just fixed during the creation of down_block, but I want the model to be a tad more modular, so that I can change any variable from the initialising section alone."
      ],
      "metadata": {
        "id": "LykoqeoaLzZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also implement batchnorms in between like so:\n",
        "\n",
        "```\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "01iyIgH-JaLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            #nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            #nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)"
      ],
      "metadata": {
        "id": "f7D3sjvjF_cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Notes about the blocks**\n",
        "****\n",
        "1. The 'down' blocks include two conv2ds. Each of them contain a convolution layer followed by a relu activation layer.\n",
        "2. Now, since the 'filters' variable is the same for both, it means that the dimension of the output of the first will be the same as that of the second - e.g: (32,32,3) --conv2d--> (32,32,filters) --conv2d--> (32,32,filters)\n",
        "3. This does not (as I initially thought) mean that the second convolution is meaningless, since it can extract further details from the basic features extracted by the first layer. This is the significance of 2 relus.\n",
        "4. Additionally, the receptive field increases over successive convolutions even if filters remain the same, because the convolution is working on the output of the first. i.e. Each point in the output of the first conv. corresponds to a 3x3 kernel on the source data\n",
        "  ****\n",
        "5. The 'up layer'"
      ],
      "metadata": {
        "id": "UrvfmfHJN-7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownBlock(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch):\n",
        "    super().__init__()\n",
        "    self.double_conv = DoubleConv(in_ch,out_ch)\n",
        "    self.down_sample = nn.MaxPool2d(2)\n",
        "\n",
        "  def forward(self,x):\n",
        "    skip_out = self.double_conv(x)\n",
        "    down_out = self.down_sample(skip_out)\n",
        "    return skip_out, down_out\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "  def __init__(self,in_ch, out_ch):\n",
        "    super().__init__()\n",
        "    self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "    self.double_conv = DoubleConv(in_ch,out_ch)\n",
        "\n",
        "  def forward(self,x,skip_connection):\n",
        "    x = self.up_sample(x)\n",
        "    '''\n",
        "    if x.shape[-2:] != skip_input.shape[-2:]:\n",
        "      diff_h = skip_input.size(2) - x.size(2)\n",
        "      diff_w = skip_input.size(3) - x.size(3)\n",
        "      skip_input = skip_input[:, :, diff_h // 2:-(diff_h - diff_h // 2), diff_w // 2:-(diff_w - diff_w // 2)]\n",
        "    x = torch.cat([x, skip_input], dim=1)\n",
        "    '''\n",
        "    print(\"upblock_xshape:\", x.shape, \"skip connection_shape:\",skip_connection.shape)\n",
        "    x = torch.cat([x,skip_connection],dim=1)\n",
        "    x = self.double_conv(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "CXl-_HE-EeBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, out_classes=10):\n",
        "        super().__init__()\n",
        "        filters = [64, 128, 256, 512, 1024]\n",
        "        # Downsampling Path\n",
        "        self.down_conv1 = DownBlock(1,64)\n",
        "        self.down_conv2 = DownBlock(64, 128)\n",
        "        self.down_conv3 = DownBlock(128, 256)\n",
        "        self.down_conv4 = DownBlock(256, 512)\n",
        "        # Bottleneck\n",
        "        self.double_conv = DoubleConv(512, 1024)\n",
        "        # Upsampling Path\n",
        "        self.up_conv4 = UpBlock(512 + 1024, 512)\n",
        "        self.up_conv3 = UpBlock(256 + 512, 256)\n",
        "        self.up_conv2 = UpBlock(128 + 256, 128)\n",
        "        self.up_conv1 = UpBlock(128 + 64, 64)\n",
        "        # Final Convolution\n",
        "        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, skip1_out = self.down_conv1(x)\n",
        "        print(\"x shape \",x.shape,\" skip1 \",skip1_out.shape)\n",
        "        x, skip2_out = self.down_conv2(x)\n",
        "        x, skip3_out = self.down_conv3(x)\n",
        "        x, skip4_out = self.down_conv4(x)\n",
        "        x = self.double_conv(x)\n",
        "        x = self.up_conv4(x, skip4_out)\n",
        "        x = self.up_conv3(x, skip3_out)\n",
        "        x = self.up_conv2(x, skip2_out)\n",
        "        x = self.up_conv1(x, skip1_out)\n",
        "        x = self.conv_last(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Get UNet model\n",
        "model = UNet()\n"
      ],
      "metadata": {
        "id": "C6X9I2fnGVnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "lst-WXuT_WIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqtH8UtyjK-e",
        "outputId": "712bca24-9291-453c-cb41-24dca53f3d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "#criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "6JztDYcnjfuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXVw7HD7wWRi",
        "outputId": "724da300-903d-4294-c7ac-9631fd8bb494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# Training loop\n",
        "num_epochs = 5  # Increase for better results\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        #Forward pass\n",
        "        outputs = model(images)  # Shape: (N, 10, H, W)\n",
        "        loss = criterion(outputs, masks)  # Ground truth masks: (N, H, W)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "sBLVwH0Ji4sX",
        "outputId": "d52e22a6-79dd-420c-d90d-33bc45991a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/5:   0%|          | 0/1500 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape  torch.Size([32, 64, 28, 28])  skip1  torch.Size([32, 64, 14, 14])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/5:   0%|          | 0/1500 [00:17<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upblock_xshape: torch.Size([32, 1024, 56, 56]) skip connection_shape: torch.Size([32, 512, 14, 14])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 56 but got size 14 for tensor number 1 in the list.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1982f1615400>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (N, 10, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ground truth masks: (N, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-861bb94285fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip4_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_conv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_conv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip4_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_conv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip3_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_conv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip2_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-843280a0813b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip_connection)\u001b[0m\n\u001b[1;32m     26\u001b[0m     '''\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"upblock_xshape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"skip connection_shape:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_connection\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 56 but got size 14 for tensor number 1 in the list."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing and visualising"
      ],
      "metadata": {
        "id": "HU9drZ4Mcx7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing loop\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for images, masks in tqdm(test_loader, desc=\"Testing\"):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "ITFRcS6Xjo5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a few predictions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "for i in range(5):  # Visualize 5 examples\n",
        "    image = X_test_tensor[i:i+1].to(device)\n",
        "    mask = Y_test_tensor[i].cpu().numpy().squeeze()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = model(image).cpu().numpy().squeeze()\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Input\")\n",
        "    plt.imshow(image.cpu().squeeze(), cmap='gray')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.imshow(pred, cmap='gray')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "2D5xmHSAjqq9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}